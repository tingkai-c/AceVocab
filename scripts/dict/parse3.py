import csv
import sqlite3
import json
import google.generativeai as genai
import os
import time
import threading
import queue  # For thread-safe queue
from tenacity import retry, stop_after_attempt, wait_exponential

# --- Configuration ---
CSV_FILE = "dictionary.csv"
DB_FILE = "vocabulary.db"
MODEL_NAME = "gemini-2.0-flash"
NUM_THREADS = 100  # Adjust based on your system and API limits

# --- LLM Prompt (Provided in the Question) ---
prompt = """
You are an expert linguistic assistant tasked with parsing a complete dictionary entry for a single English headword into a structured JSON format. You will analyze the provided text block, identify distinct senses, extract definitions and examples, AND enhance the entry by translating or generating examples where appropriate, and creating a concise summary translation.

**Your Tasks:**

1. **Parse Senses:** Identify all distinct senses (meanings, idioms, phrasal verbs) within the input text. Determine the order they appear in. You can merge different usages into one sense with multiple examples if they have close meanings.
2. **Extract Core Info:** For each sense, extract the primary Chinese translation (`translation_chn`), English definition (`definition_eng`), and infer the part of speech (`part_of_speech`).
3. **Process Examples:**
    - Extract all examples associated with the *correct* sense.
    - If there isn't an example, make one following the rules below.
    - For each example, extract the `phrase_marker` (leading phrase, **without trailing colon**), `sentence_eng`, and any existing `sentence_chn`.
    - **Enhance Examples:**
        - If an extracted example has `sentence_eng` but `sentence_chn` is missing or absent in the original text, **provide a high-quality Chinese translation** for `sentence_chn`. Mark its `source` as `"translated"`.
        - If a sense lacks clear or sufficient examples in the original text, **generate 1-2 simple, clear, and relevant example sentence pairs** (`sentence_eng` and `sentence_chn`) that accurately illustrate the meaning of *that specific sense*. Mark their `source` as `"generated"`.
        - If an example has both English and Chinese in the original text, keep them and mark its `source` as `"original"`.
4. **Generate Summary:** After processing all senses, create a `short_translation_summary` string. This string should contain the primary `translation_chn` from each identified sense, separated by a semicolon and space (`;` ). (e.g., "帳戶；認為是；描述").
5. **Format Output:** Structure the entire result as a single JSON object according to the specifications below.

**Output JSON Structure:**

The top-level JSON object must contain the following keys:

- `short_translation_summary`: (String) The concise summary of Chinese translations no longer than 15 characters generated by you. Feel free to remove any meaning that isn't frequently used. (e.g., "主要意思一；主要意思二；主要意思三").
- `senses`: (Array of Objects) Ordered array of identified senses. Each sense object must have:
    - `sense_order`: (Integer) 0-based order index.
    - `translation_chn`: (String or Null) Extracted Chinese translation(s).
    - `definition_eng`: (String or Null) Extracted English definition.
    - `part_of_speech`: (String or Null) Inferred part of speech.
    - `examples`: (Array of Objects) Ordered array of examples for this sense. Each example object must have:
        - `example_order`: (Integer) 0-based order index within the sense.
        - `phrase_marker`: (String or Null) Leading phrase (no trailing colon). This marker marks what phrase is used in the example sentence. Can be null.
        - `sentence_eng`: (String) English example sentence using the phrase or just the word itself.
        - `sentence_chn`: (String or Null) Chinese example sentence (original, translated, or generated).
        - `source`: (String) Must be one of: `"original"`, `"translated"`, `"generated"`.

**Instructions & Constraints:**

- Adhere strictly to the JSON structure and field names.
- Ensure `sense_order` and `example_order` are sequential 0-based integers.
- Prioritize accuracy when translating or generating content. Examples should clearly reflect the specific sense meaning.
- Feel free to add in definition, senses ,examples if they are missing.
- If information is genuinely missing or cannot be determined (e.g., `part_of_speech`), use `null`.
- Output ONLY the JSON object. Do not include any explanations, apologies, or conversational text before or after the JSON.
- A sense must not have over 4 examples.
- Do NOT use simplified Chinese. You MUST use traditional chinese.
- Your output MUST be a valid JSON string, and NOT any HTML format.

**Example Snippet of Expected Output:**

{
  "short_translation_summary": "帳戶；認為是；描述",
  "senses": [
    {
      "sense_order": 0,
      "translation_chn": "帳戶，戶口",
      "definition_eng": "an arrangement with a bank...",
      "part_of_speech": "noun",
      "examples": [
        {
          "example_order": 0,
          "phrase_marker": "close an account",
          "sentence_eng": "I withdrew my money...",
          "sentence_chn": "我取了錢並要求銀行關閉我的帳戶。",
          "source": "translated"
        },
        {
          "example_order": 1,
          "phrase_marker": null,
          "sentence_eng": "Check your account balance online.",
          "sentence_chn": "在線查詢您的帳戶餘額。",
          "source": "generated"
        }
      ]
    },
    // ... other senses
  ]
}
Now, parse the following complete dictionary entry text block, applying all extraction, translation, generation, and summarization rules:
"""


# --- Initialize Google Generative AI API ---
def initialize_genai():
    API_KEY = os.environ.get("GEMINI_API_KEY")
    if not API_KEY:
        raise ValueError("Please set the GEMINI_API_KEY environment variable.")
    genai.configure(api_key=API_KEY)
    return genai.GenerativeModel(MODEL_NAME)


# --- Database Setup (Modified) ---
def create_database_schema(db_file):
    conn = sqlite3.connect(db_file)
    cursor = conn.cursor()

    # Add the short_translation_summary column to the words table if it doesn't exist
    try:
        cursor.execute("ALTER TABLE words ADD COLUMN short_translation_summary TEXT")
        print("Added short_translation_summary column to words table")
    except sqlite3.OperationalError:
        print("short_translation_summary column already exists in words table")

    cursor.execute(
        """
        CREATE TABLE IF NOT EXISTS senses (
            sense_id INTEGER PRIMARY KEY AUTOINCREMENT,
            word_id INTEGER NOT NULL,  -- FK to the existing words table
            sense_order INTEGER NOT NULL,
            translation_chn TEXT,
            definition_eng TEXT,
            part_of_speech TEXT,
            original_input_text TEXT,
            FOREIGN KEY (word_id) REFERENCES words (id) ON DELETE CASCADE
        )
    """
    )

    cursor.execute(
        """
        CREATE TABLE IF NOT EXISTS examples (
            example_id INTEGER PRIMARY KEY AUTOINCREMENT,
            sense_id INTEGER NOT NULL,
            example_order INTEGER NOT NULL,
            phrase_marker TEXT,
            sentence_eng TEXT NOT NULL,
            sentence_chn TEXT,
            example_source TEXT NOT NULL DEFAULT 'original',
            FOREIGN KEY (sense_id) REFERENCES senses (sense_id) ON DELETE CASCADE
        )
    """
    )

    conn.commit()
    conn.close()


# --- Retry Decorator for API Calls ---
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def generate_content_with_retry(model, full_prompt):
    response = model.generate_content(full_prompt)
    return response


# --- Worker Function for Threading ---
def worker(queue, model):
    while True:
        item = queue.get()
        if item is None:
            break  # Sentinel value to signal thread to exit

        word, description = item

        # --- Database Connection within Thread ---
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()

        try:
            print(f"Thread {threading.current_thread().name} processing: {word}")

            # --- Check if short_translation_summary already exists ---
            cursor.execute(
                "SELECT short_translation_summary FROM words WHERE word = ?", (word,)
            )
            result = cursor.fetchone()
            if result and result[0]:
                print(
                    f"Thread {threading.current_thread().name} Skipping {word} - short_translation_summary already exists"
                )
                continue  # Skip to the next word

            # --- Construct LLM Prompt ---
            full_prompt = prompt + f"\n**Input Text Block:**\n```\n{description}\n```"

            # --- Call the LLM with Retry ---
            try:
                response = generate_content_with_retry(model, full_prompt)
                json_string = response.text  # Extracting the JSON string
                # --- Strip JSON tags ---
                json_string = (
                    json_string.strip().replace("```json", "").replace("```", "")
                )

                print(
                    f"Thread {threading.current_thread().name} LLM response: {json_string}"
                )  # Printing the raw response
            except Exception as e:
                print(
                    f"Thread {threading.current_thread().name} LLM API Error after retry for {word}: {e}"
                )
                continue  # Skip to next word

            # --- Parse JSON Response ---
            try:
                data = json.loads(json_string)
            except json.JSONDecodeError as e:
                print(
                    f"Thread {threading.current_thread().name} JSON Decode Error for {word}: {e}"
                )
                print(
                    f"Thread {threading.current_thread().name} Failing JSON String: {json_string}"
                )
                continue  # skip to next word

            # --- Database Insertion (Modified)---
            try:
                # 1. Get the word_id from the existing words table
                cursor.execute("SELECT id FROM words WHERE word = ?", (word,))
                result = cursor.fetchone()
                if not result:
                    print(
                        f"Thread {threading.current_thread().name} Word not found in 'words' table: {word}"
                    )
                    continue  # Skip to the next word if not found
                word_id = result[0]

                # 2. Update short translation summary
                short_translation_summary = data.get(
                    "short_translation_summary", None
                )  # Handle missing field
                cursor.execute(
                    "UPDATE words SET short_translation_summary = ? WHERE id = ?",
                    (short_translation_summary, word_id),
                )  # update the short_translation in word table instead of creating another one.

                # 3. Insert Senses and Examples
                for sense_data in data.get("senses", []):  # Handle missing 'senses' key
                    sense_order = sense_data.get(
                        "sense_order", 0
                    )  # Handle missing sense_order
                    translation_chn = sense_data.get("translation_chn")
                    definition_eng = sense_data.get("definition_eng")
                    part_of_speech = sense_data.get("part_of_speech")
                    original_input_text = sense_data.get("original_input_text")

                    cursor.execute(
                        """
                        INSERT INTO senses (word_id, sense_order, translation_chn, definition_eng, part_of_speech, original_input_text)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """,
                        (
                            word_id,
                            sense_order,
                            translation_chn,
                            definition_eng,
                            part_of_speech,
                            original_input_text,
                        ),
                    )

                    sense_id = cursor.lastrowid

                    for example_data in sense_data.get(
                        "examples", []
                    ):  # Handle missing 'examples' key
                        example_order = example_data.get(
                            "example_order", 0
                        )  # Handle missing example_order
                        phrase_marker = example_data.get("phrase_marker")
                        sentence_eng = example_data.get("sentence_eng")
                        sentence_chn = example_data.get("sentence_chn")
                        example_source = example_data.get(
                            "source", "original"
                        )  # Default to "original"

                        cursor.execute(
                            """
                            INSERT INTO examples (sense_id, example_order, phrase_marker, sentence_eng, sentence_chn, example_source)
                            VALUES (?, ?, ?, ?, ?, ?)
                        """,
                            (
                                sense_id,
                                example_order,
                                phrase_marker,
                                sentence_eng,
                                sentence_chn,
                                example_source,
                            ),
                        )

                conn.commit()  # Commit after processing each word
                print(
                    f"Thread {threading.current_thread().name} Successfully processed and stored: {word}"
                )

            except Exception as db_error:
                print(
                    f"Thread {threading.current_thread().name} Database Error for {word}: {db_error}"
                )
                conn.rollback()  # Rollback on any error

        finally:
            conn.close()  # Always close the connection
        queue.task_done()  # Signal the queue that the task is complete


# --- Data Processing and Insertion (Modified for Threading) ---
def process_csv_threaded(csv_file, db_file, num_threads):
    # --- Create a thread-safe queue ---
    task_queue = queue.Queue()

    # --- Read the CSV and populate the queue ---
    with open(csv_file, "r", encoding="utf-8") as file:
        reader = csv.reader(file)
        next(reader)  # Skip header row

        for row in reader:
            word, description = row
            if " " in word:  # Skip multi-word entries
                print(f"Skipping multi-word entry: {word}")
                continue
            task_queue.put((word, description))

    # --- Initialize Google Generative AI model outside threads ---
    model = initialize_genai()

    # --- Create worker threads ---
    threads = []
    for i in range(num_threads):
        t = threading.Thread(
            target=worker, args=(task_queue, model), name=f"Thread-{i + 1}"
        )
        threads.append(t)
        t.daemon = True  # Allow main thread to exit even if workers are running
        t.start()

    # --- Wait for all tasks to be done ---
    task_queue.join()

    # --- Signal the workers to exit ---
    for i in range(num_threads):
        task_queue.put(None)  # Add sentinel values to stop the workers

    # --- Wait for all threads to finish ---
    for t in threads:
        t.join()

    print("Threaded processing complete.")


# --- Main ---
if __name__ == "__main__":
    create_database_schema(
        DB_FILE
    )  # Ensure the senses and examples table are created even if words already exist
    process_csv_threaded(CSV_FILE, DB_FILE, NUM_THREADS)
